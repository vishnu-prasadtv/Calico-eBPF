# Calico-eBPF

eBPF is a mechanism for running small programs in a virtual machine embedded within the Linux kernel on a Linux host (such as a Kubernetes node). These are attached to hooks, which are triggered when some event occurs. This allows the behaviour of the kernel to be (sometimes heavily) customised. The pre-defined hooks include network events, system calls, function entry and exit points, kernel tracepoints, and others.

eBPF stands for “extended Berkeley Packet Filter”. The Berkeley Packet Filter was an earlier, more specialised virtual machine that was tailored for filtering packets. Tools such as tcpdump use this “classic” BPF virtual machine to select packets that should be sent to userspace for analysis. eBPF is a considerably extended version of BPF that is suitable for general-purpose use inside the kernel. While the name has stuck, eBPF can be used for a lot more than just packet filtering.

As noted previously, later you’ll learn about how Calico uses eBPF and changes the behaviour of the Linux kernel, and the benefits this provides. First, you need to understand how the kernel’s networking usually works.

Netfilter is a framework provided by the Linux kernel allowing networking-related operations to be implemented via customised hooks (or handlers). The excellent diagram below (courtesy of Jan Engelhardt, used with permission) helps to visualise the flow of network packets through Netfilter on a single Linux host (including on Kubernetes nodes). Click through for an SVG version if you're finding it hard to see the detail:

## Netfilter-packet-flow

![image](https://github.com/user-attachments/assets/41bbadca-5b9e-42cc-adf2-3a6fa2ee77b9)

## Key Components and Packet Flow

### PREROUTING Hook:
Function: Intercepts incoming packets immediately after they arrive and before any routing decisions are made.
Purpose: Allows for initial packet manipulations, such as destination NAT (DNAT), where the destination address of the packet can be altered before routing.

### Routing Decision:
Function: Determines whether the packet is destined for the local system or should be forwarded to another interface.
Purpose: Guides the packet to the appropriate processing path based on its destination.

### INPUT Hook:
Function: Processes packets destined for the local system after routing decisions confirm local delivery.
Purpose: Enables filtering and processing of incoming packets intended for local applications.

### FORWARD Hook:
Function: Handles packets that are routed through the system but are not destined for it.
Purpose: Allows for filtering and manipulation of packets being forwarded to another network interface.

### OUTPUT Hook:
Function: Manages packets generated by the local system before they are routed.
Purpose: Provides an opportunity to alter or filter outgoing packets from local processes.

### POSTROUTING Hook:
Function: Processes packets after routing decisions have been made, just before they leave the system.
Purpose: Commonly used for source NAT (SNAT), where the source address of the packet is modified before it exits the system.


## Packet Flow Scenarios
### Incoming Packets:
- Upon arrival, packets hit the PREROUTING hook.
- A routing decision is made to determine if the packet is for the local system or should be forwarded.
- If destined for the local system, the packet traverses the INPUT hook and is then delivered to the appropriate local process.
- If the packet is to be forwarded, it passes through the FORWARD hook and then the POSTROUTING hook before being transmitted out.


### Outgoing Packets:
- Packets generated by local processes enter the OUTPUT hook first.
- After routing decisions, they proceed to the POSTROUTING hook before being sent out to the network.

## Tables and Chains in Netfilter

Netfilter utilizes tables to organize rules applied at various hooks:

### Filter Table:  
*Chains*: INPUT, FORWARD, OUTPUT  
*Purpose*: General packet filtering to ACCEPT or DROP packets.  

### NAT Table:  
*Chains*: PREROUTING, POSTROUTING, OUTPUT  
*Purpose*: Handles NAT operations like DNAT and SNAT.  

### Mangle Table:  
*Chains*: PREROUTING, POSTROUTING, INPUT, OUTPUT, FORWARD  
*Purpose*: Specialized packet alterations, such as modifying the Type of Service (ToS) field.  

### Raw Table:  
*Chains*: PREROUTING, OUTPUT  
*Purpose*: Exempting certain packets from connection tracking.  

Before and after the main coloured area of the diagram are ingress and egress qdiscs (short for queuing disciplines). They are a scheduler component of the Linux Traffic Control Framework (tc).

The framework was originally designed for traffic shaping. Its main data structure is a tree of qdiscs. Some qdiscs support attaching eBPF programs; this was originally intended for advanced packet classification but the current implementation can do a lot more, including modifying the packet, redirecting it or dropping it.

Since not all qdiscs support eBPF, the kernel provides a no-op qdisc called clsact (classifier and action), which is solely for attaching an eBPF program. If no eBPF program is attached, this qdisc doesn’t do anything. It can coexist with other qdiscs.

While the eBPF virtual machine is the same for each type of hook, the capabilities offered to the eBPF program at each of the hooks vary considerably. Since loading programs into the kernel could be dangerous, the kernel runs all programs through a very strict static verifier. The verifier sandboxes the program, ensuring, amongst other things, that it can only access allowed parts of memory and that it terminates quickly.

The kernel exposes the capabilities of each hook via “helper functions”. For example, the tc hook has a helper function to resize the packet, but that helper would not be available in a tracing hook. One of the challenges of working with eBPF is that different kernel versions support different helpers and lack of a helper can make it impossible to implement a particular feature.

## eBPF Maps

For a program to be useful, generally it needs to be able to store data. For example, consider the case of an eBPF program that handles networking. Without the ability to store data, every packet would need to be considered in isolation (known as stateless forwarding). With the ability to store data, eBPF programs can cross-reference decisions made in other programs. This is known as stateful forwarding.

eBPF maps are a class of data structure available to eBPF programs. Using eBPF maps allows a program to keep state between different runs of an eBPF program. It also allows sharing data between eBPF programs, and with userspace applications.

eBPF maps can be considered as a type of key/value store. They are available in various types. This list is developing. You can view it in the Linux source code here.

You do not need to remember this list, but for interest and understanding here is the enum at the time of writing this course:
```
enum bpf_map_type {
	BPF_MAP_TYPE_UNSPEC,
	BPF_MAP_TYPE_HASH,
	BPF_MAP_TYPE_ARRAY,
	BPF_MAP_TYPE_PROG_ARRAY,
	BPF_MAP_TYPE_PERF_EVENT_ARRAY,
	BPF_MAP_TYPE_PERCPU_HASH,
	BPF_MAP_TYPE_PERCPU_ARRAY,
	BPF_MAP_TYPE_STACK_TRACE,
	BPF_MAP_TYPE_CGROUP_ARRAY,
	BPF_MAP_TYPE_LRU_HASH,
	BPF_MAP_TYPE_LRU_PERCPU_HASH,
	BPF_MAP_TYPE_LPM_TRIE,
	BPF_MAP_TYPE_ARRAY_OF_MAPS,
	BPF_MAP_TYPE_HASH_OF_MAPS,
	BPF_MAP_TYPE_DEVMAP,
	BPF_MAP_TYPE_SOCKMAP,
	BPF_MAP_TYPE_CPUMAP,
	BPF_MAP_TYPE_XSKMAP,
	BPF_MAP_TYPE_SOCKHASH,
	BPF_MAP_TYPE_CGROUP_STORAGE,
	BPF_MAP_TYPE_REUSEPORT_SOCKARRAY,
	BPF_MAP_TYPE_PERCPU_CGROUP_STORAGE,
	BPF_MAP_TYPE_QUEUE,
	BPF_MAP_TYPE_STACK,
	BPF_MAP_TYPE_SK_STORAGE,
	BPF_MAP_TYPE_DEVMAP_HASH,
	BPF_MAP_TYPE_STRUCT_OPS,
	BPF_MAP_TYPE_RINGBUF,
	BPF_MAP_TYPE_INODE_STORAGE,
	BPF_MAP_TYPE_TASK_STORAGE,
};
```
As you can see, there are many types, but common data structures such as hashes, arrays, queues, and ring buffers are available, and the right eBPF map type depends on the algorithm that suits the programming task you are trying to solve.

## XDP

eXpress Data Path, or XDP, is a suite of eBPF hooks that allow the attachment of eBPF programs at a very early point in the kernel receive processing path. Additional eBPF programs can still be attached to later hooks as usual, as shown:

![image](https://github.com/user-attachments/assets/8d155351-4f64-41bc-a266-0b13a8a2e537)

Depending on specific network hardware and kernel version, there are three places in the pipeline where an XDP program can be attached:

- offloaded all the way to the NIC (blazing fast, processed in hardware)
- in the NIC driver (very fast, before it reaches the main Linux kernel network stack)
- after the NIC driver, in the kernel’s “generic” processing (early in the main Linux kernel network stack)
Since the hook is placed before any memory allocation is necessary, XDP can achieve very high performance. The eBPF program is still sandboxed. It is allowed to edit the packet data and make a forwarding decision.

## Advantages of eBPF

Well-optimised code running inside the Linux kernel can perform better than code running in userspace. However, without eBPF it would be necessary to change kernel source code or use loaded kernel modules in order to implement the desired functionality.

It is challenging and slow to get a change to the Linux kernel source code approved and implemented.

Loading kernel modules is easier, but the resulting code is not sandboxed. Furthermore, it must be recompiled for each kernel.

eBPF code, in contrast to these other options, allows the performance benefits of running code in the kernel whilst still offering security, via sandboxing, and development agility by allowing code running in the kernel to be updated without the caveats identified above.

## Requirements for eBPF

The Linux kernel has included an eBPF virtual machine since version 3.18. However, subsequent versions have added additional features.

Calico is able to make use of eBPF on any of:
- Ubuntu 22.04 (or Ubuntu 18.04.4+, which has an updated kernel).
- Red Hat v8.2 with Linux kernel v4.18.0-193 or above (Red Hat have backported the required features to that build).
- Another supported distribution with Linux kernel v5.3 or above.


